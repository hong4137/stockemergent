"""
Stock Sentinel â€” News Collector
ë‰´ìŠ¤/ê³µì‹œ ìˆ˜ì§‘: Google News RSS, Finnhub, SEC EDGAR
"""
import feedparser
import requests
import json
import re
from datetime import datetime, timedelta
from typing import List, Dict, Optional
import time

import sys, os
sys.path.insert(0, os.path.join(os.path.dirname(__file__), ".."))
from config.settings import (
    WATCHMAP, NEWS_RSS_FEEDS, FINNHUB_API_KEY,
    BREAKING_KEYWORDS, POSITIVE_KEYWORDS, NEGATIVE_KEYWORDS
)
from storage.database import save_news


def collect_google_news(ticker: str, hours: int = 24) -> List[Dict]:
    """Google News RSSë¡œ ì¢…ëª© ê´€ë ¨ ë‰´ìŠ¤ ìˆ˜ì§‘"""
    watch = WATCHMAP.get(ticker)
    if not watch:
        return []
    
    results = []
    
    # í‹°ì»¤ + íšŒì‚¬ëª…ìœ¼ë¡œ ê°ê° ê²€ìƒ‰
    queries = [ticker, watch.name]
    
    for query in queries:
        url = f"https://news.google.com/rss/search?q={query}+stock&hl=en-US&gl=US&ceid=US:en"
        try:
            feed = feedparser.parse(url)
            cutoff = datetime.utcnow() - timedelta(hours=hours)
            
            for entry in feed.entries[:20]:  # ìµœëŒ€ 20ê°œ
                # ë°œí–‰ ì‹œê°„ íŒŒì‹±
                pub_time = None
                if hasattr(entry, 'published_parsed') and entry.published_parsed:
                    pub_time = datetime(*entry.published_parsed[:6])
                else:
                    pub_time = datetime.utcnow()
                
                if pub_time < cutoff:
                    continue
                
                title = entry.get('title', '')
                link = entry.get('link', '')
                summary = entry.get('summary', '')
                
                # í‚¤ì›Œë“œ ë§¤ì¹­
                text = (title + " " + summary).lower()
                matched_keywords = [kw for kw in (watch.keywords + [ticker, watch.name])
                                   if kw.lower() in text]
                
                # ì„¼í‹°ë©˜íŠ¸ ê°„ì´ íŒì •
                sentiment = _simple_sentiment(text)
                
                article = {
                    "ticker": ticker,
                    "timestamp": pub_time.isoformat(),
                    "title": _clean_html(title),
                    "summary": _clean_html(summary)[:500],
                    "url": link,
                    "source": "google_news",
                    "source_type": "news",
                    "sentiment": sentiment,
                    "keywords_matched": matched_keywords,
                }
                results.append(article)
                
                # DB ì €ì¥
                save_news(**article)
                
            time.sleep(1)  # Rate limit
            
        except Exception as e:
            print(f"  âš ï¸ Google News ìˆ˜ì§‘ ì˜¤ë¥˜ ({query}): {e}")
    
    # URL ê¸°ì¤€ ì¤‘ë³µ ì œê±°
    seen = set()
    unique = []
    for r in results:
        if r['url'] not in seen:
            seen.add(r['url'])
            unique.append(r)
    
    print(f"  ğŸ“° Google News [{ticker}]: {len(unique)}ê±´ ìˆ˜ì§‘")
    return unique


def collect_finnhub_news(ticker: str, hours: int = 72) -> List[Dict]:
    """Finnhub APIë¡œ ë‰´ìŠ¤ ìˆ˜ì§‘ (ë¬´ë£Œ í‹°ì–´)"""
    if not FINNHUB_API_KEY:
        print(f"  âš ï¸ Finnhub API í‚¤ ë¯¸ì„¤ì • â€” ê±´ë„ˆëœ€")
        return []
    
    watch = WATCHMAP.get(ticker)
    results = []
    
    now = datetime.utcnow()
    from_date = (now - timedelta(hours=hours)).strftime("%Y-%m-%d")
    to_date = now.strftime("%Y-%m-%d")
    
    try:
        url = f"https://finnhub.io/api/v1/company-news"
        params = {
            "symbol": ticker,
            "from": from_date,
            "to": to_date,
            "token": FINNHUB_API_KEY,
        }
        resp = requests.get(url, params=params, timeout=10)
        resp.raise_for_status()
        articles = resp.json()
        
        for a in articles[:30]:
            pub_time = datetime.fromtimestamp(a.get('datetime', 0))
            title = a.get('headline', '')
            summary = a.get('summary', '')
            link = a.get('url', '')
            source_name = a.get('source', 'finnhub')
            
            text = (title + " " + summary).lower()
            matched_keywords = []
            if watch:
                matched_keywords = [kw for kw in watch.keywords if kw.lower() in text]
            
            sentiment = _simple_sentiment(text)
            
            article = {
                "ticker": ticker,
                "timestamp": pub_time.isoformat(),
                "title": title,
                "summary": summary[:500],
                "url": link,
                "source": f"finnhub:{source_name}",
                "source_type": "news",
                "sentiment": sentiment,
                "keywords_matched": matched_keywords,
            }
            results.append(article)
            save_news(**article)
        
        print(f"  ğŸ“° Finnhub [{ticker}]: {len(results)}ê±´ ìˆ˜ì§‘")
        
    except Exception as e:
        print(f"  âš ï¸ Finnhub ìˆ˜ì§‘ ì˜¤ë¥˜: {e}")
    
    return results


def collect_sec_edgar(ticker: str, company_name: str = None) -> List[Dict]:
    """SEC EDGARì—ì„œ ìµœê·¼ Filing í™•ì¸"""
    watch = WATCHMAP.get(ticker)
    if not company_name and watch:
        company_name = watch.name
    
    results = []
    
    try:
        # EDGAR Full-Text Search API
        url = "https://efts.sec.gov/LATEST/search-index"
        params = {
            "q": f'"{ticker}"',
            "dateRange": "custom",
            "startdt": (datetime.utcnow() - timedelta(days=7)).strftime("%Y-%m-%d"),
            "enddt": datetime.utcnow().strftime("%Y-%m-%d"),
            "forms": "8-K,10-Q,10-K,4",
        }
        
        headers = {"User-Agent": "StockSentinel research@example.com"}
        resp = requests.get(url, params=params, headers=headers, timeout=10)
        
        if resp.status_code == 200:
            data = resp.json()
            hits = data.get('hits', {}).get('hits', [])
            
            for hit in hits[:10]:
                source = hit.get('_source', {})
                filing = {
                    "ticker": ticker,
                    "timestamp": source.get('file_date', datetime.utcnow().isoformat()),
                    "title": f"[SEC {source.get('form_type', 'Filing')}] {source.get('entity_name', '')}",
                    "summary": source.get('display_names', [''])[0] if source.get('display_names') else '',
                    "url": f"https://www.sec.gov/cgi-bin/browse-edgar?action=getcompany&CIK={ticker}&type=&dateb=&owner=include&count=10",
                    "source": "sec_edgar",
                    "source_type": "filing",
                    "sentiment": "neutral",
                    "keywords_matched": [source.get('form_type', '')],
                }
                results.append(filing)
                save_news(**filing)
            
            print(f"  ğŸ“‹ SEC EDGAR [{ticker}]: {len(results)}ê±´ ìˆ˜ì§‘")
        else:
            print(f"  âš ï¸ SEC EDGAR ì‘ë‹µ ì½”ë“œ: {resp.status_code}")
            
    except Exception as e:
        print(f"  âš ï¸ SEC EDGAR ìˆ˜ì§‘ ì˜¤ë¥˜: {e}")
    
    return results


def collect_all_news(ticker: str) -> Dict:
    """ëª¨ë“  ë‰´ìŠ¤ ì†ŒìŠ¤ì—ì„œ ì¢…ëª© ê´€ë ¨ ë‰´ìŠ¤ í†µí•© ìˆ˜ì§‘"""
    print(f"\nğŸ“¡ ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹œì‘: {ticker}")
    
    all_news = {
        "google_news": collect_google_news(ticker),
        "finnhub": collect_finnhub_news(ticker),
        "sec_edgar": collect_sec_edgar(ticker),
    }
    
    total = sum(len(v) for v in all_news.values())
    print(f"  âœ… ì´ {total}ê±´ ìˆ˜ì§‘ ì™„ë£Œ")
    
    return all_news


# ============================================================
# ìœ í‹¸ë¦¬í‹°
# ============================================================

def _simple_sentiment(text: str) -> str:
    """ê°„ë‹¨í•œ í‚¤ì›Œë“œ ê¸°ë°˜ ì„¼í‹°ë©˜íŠ¸ íŒì •"""
    text_lower = text.lower()
    
    pos_count = sum(1 for kw in POSITIVE_KEYWORDS if kw.lower() in text_lower)
    neg_count = sum(1 for kw in NEGATIVE_KEYWORDS if kw.lower() in text_lower)
    
    if pos_count > neg_count + 1:
        return "positive"
    elif neg_count > pos_count + 1:
        return "negative"
    return "neutral"


def has_breaking_keywords(text: str) -> bool:
    """í˜„ì¥ì„± í‚¤ì›Œë“œ í¬í•¨ ì—¬ë¶€"""
    text_lower = text.lower()
    return any(kw.lower() in text_lower for kw in BREAKING_KEYWORDS)


def _clean_html(text: str) -> str:
    """HTML íƒœê·¸ ì œê±°"""
    return re.sub(r'<[^>]+>', '', text).strip()


# ============================================================
# í…ŒìŠ¤íŠ¸
# ============================================================
if __name__ == "__main__":
    from storage.database import init_db
    init_db()
    
    result = collect_all_news("AMAT")
    
    for source, articles in result.items():
        print(f"\n--- {source} ---")
        for a in articles[:3]:
            print(f"  [{a['sentiment']}] {a['title'][:80]}")
